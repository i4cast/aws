{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, deploy and review ML algorithm/model VBfFA (Variational Bayesian filtering Factor Analysis) from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the algorithm  \n",
    "  \n",
    "The variational Bayesian filtering factor analysis (VBfFA) algorithm/model is a filter (of dimension-reduction, or rank-reduction) to extract a number of ever-evolving unobserved common factors, or signals from common sources, underlying and influencing a large number of related time-series data.\n",
    "  \n",
    "Relevant examples of time-series data include: economic indicators in a nation, a region, or an international economic sector; prices of assets in a national, regional or global asset class(es), or marketplace; performance measurement time-series with various lags related to a business marketing campaign; and time-series signals from an array of radar or sonar sensors tracking a number of moving targets; etc.\n",
    "  \n",
    "By applying (variational) Bayesian filtering (instead of traditional moving/rolling data windows for frequentist time-dependent analysis), the VBfFA algorithm is able to update predictions with only the newly arrived time-series data point (instead of all data points in the data window); to speed up, as a result, real-time prediction process; to predict underlying changes in time-series early; and to avoid over- or under-fitting by setting a reasonable “estimation error reduction target”.\n",
    "  \n",
    "In addition to serving as a stand-alone filtering package for time-varying factor analysis on multiple time-series data, the VBfFA algorithm will be employed as the underlying factor analysis engine of other machine learning packages here introduced earlier by i4cast LLC: LMDFM (long memory dynamic factor model); YWpcAR (Yule-Walker-PCA autoregressive model); LMVAR (long memory vector autoregressive model); and CTVARF (continuously trained vector autoregressive forecast model).\n",
    "  \n",
    "Current version of the VBfFA algorithm estimates: time-series of posterior from the (variational) Bayesian filtering; predicted values and time-dependent variances of common factors (or common signals) and time-dependent factor loadings; predicted time-dependent (or time-varying) variance-covariance matrix of multiple time-series; and evaluation scores of the predicted time-series of variance-covariance matrix.\n",
    "  \n",
    "A notable application of the VBfFA estimates, detecting timely changes in time-dependent variance-covariance matrix of financial instruments, presented in a particular form, can serve as an early warning system indicating potential troubles in the financial market.\n",
    "  \n",
    "This VBfFA module implements VBfFA formulation published by Figure 1 through Figure 4 in a paper in journal of Quantitative Finance, https://doi.org/10.1080/14697688.2016.1268708, or, https://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1268708, or, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Publication.pdf, or manuscript of the publication, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Manuscript.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publications on variational Bayesian filtering factor analysis (VBfFA) modeling  \n",
    "\n",
    "HFL & CF. (2017) \"Online learning of time-varying stochastic factor structure by variational sequential Bayesian factor analysis\", Quantitative Finance, Vol. 17 (8), pp. 1277-1304. Publication: https://doi.org/10.1080/14697688.2016.1268708, or, https://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1268708, or, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Publication.pdf. Manuscript: https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Manuscript.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook\n",
    "   \n",
    "This sample notebook shows you how to train, tune, deploy and understand a custom ML algorithm/model: [variational Bayesian filtering factor analysis (VBfFA)](https://aws.amazon.com/marketplace/pp/prodview-vdwcbntcsnu72?sr=0-5&ref_=beagle&applicationId=AWSMPContessa), guided by common practices to [Use Algorithm and Model Package Resources](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-buy.html).\n",
    "   \n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    1. or your AWS account has a subscription to [variational Bayesian filtering factor analysis (VBfFA)](https://aws.amazon.com/marketplace/pp/prodview-vdwcbntcsnu72?sr=0-5&ref_=beagle&applicationId=AWSMPContessa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "    1. [Subscription](#1.1.-Subscription)\n",
    "    1. [Prepare relevant environment](#1.2.-Prepare-relevant-environment)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "    1. [Dataset format expected by the algorithm](#2.1.-Dataset-format-expected-by-the-algorithm)\n",
    "    1. [Configure and visualize training dataset](#2.2.-Configure-and-visualize-training-dataset)\n",
    "    1. [Upload datasets to Amazon S3](#2.3.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Train a machine learning model](#3.-Train-a-machine-learning-model)\n",
    "    1. [Set hyperparameters](#3.1.-Set-hyperparameters)\n",
    "    1. [Train a model](#3.2.-Train-a-model)\n",
    "    1. [Update a model](#3.3.-Update-a-model-with-\"trained-model-retrieval\")\n",
    "1. [Tune your model (optional)](#4.-Tune-your-model-(optional))\n",
    "    1. [Tuning Guidelines](#4.1.-Tuning-guidelines)\n",
    "    1. [Define Tuning configuration](#4.2.-Define-tuning-configuration)\n",
    "    1. [Run a model tuning job](#4.3.-Run-a-model-tuning-job)\n",
    "1. [Deploy model and verify results](#5.-Deploy-model-and-verify-results)\n",
    "    1. [Trained or tuned model](#5.1.-Trained-or-tuned-model)\n",
    "    1. [Deploy trained or tuned model](#5.2.-Deploy-trained-or-tuned-model)\n",
    "    1. [Create input payload](#5.3.-Create-input-payload)\n",
    "    1. [Perform real-time inference](#5.4.-Perform-real-time-inference)\n",
    "1. [Perform Batch inference](#6.-Perform-batch-inference)\n",
    "    1. [Batch transform](#6.1.-Batch-transform)\n",
    "    1. [Delete the model](#6.2.-Delete-the-model)\n",
    "1. [Model review by using Transformer (optional)](#7.-Model-review-by-using-Transformer-(optional))\n",
    "    1. [VBfFA predictions and goodness scores](#7.1.-VBfFA-predictions-and-goodness-scores-of-the-predictions)\n",
    "    1. [Select prediction or score for review](#7.2.-Select-prediction-or-score-for-review)\n",
    "    1. [Model output review with Transformer](#7.3.-Model-output-review-with-Transformer)\n",
    "1. [Clean-up](#8.-Clean-up)\n",
    "    1. [Delete endpoint and model](#8.1.-Delete-endpoint-and-model)\n",
    "    1. [Unsubscribe to the listing (optional)](#8.2.-Unsubscribe-to-the-listing-(optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage instructions\n",
    "\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sagemaker Notebook\n",
    "\n",
    "For readers who like to review how to use Sagemaker Notebook in general, following Sagemaker documentation pages are best resources.  \n",
    "    [Get Started with Amazon SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)  \n",
    "    [Step 1: Create an Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)  \n",
    "    [Step 2: Create a Jupyter Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)  \n",
    "    [Step 3: Download, Explore, and Transform a Dataset](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html)  \n",
    "    [Step 4: Train a Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html)  \n",
    "    [Step 5: Deploy the Model to Amazon EC2](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html)  \n",
    "    [Step 6: Evaluate the Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html)  \n",
    "    [Step 7: Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Subscription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To subscribe to the algorithm:\n",
    "  \n",
    "1. Open the algorithm listing page,\n",
    "[variational Bayesian filtering factor analysis (VBfFA)](https://aws.amazon.com/marketplace/pp/prodview-vdwcbntcsnu72?sr=0-5&ref_=beagle&applicationId=AWSMPContessa)\n",
    "1. On the AWS Marketplace listing,  click on **Continue to subscribe** button.\n",
    "1. On the **Subscribe to this software** page, review and click on **\"Accept Offer\"** if you agree with EULA, pricing, and support terms. \n",
    "1. Once you click on **Continue to configuration button** and then choose a **region**, you will see a **Product Arn**. This is the algorithm ARN that you need to specify while training a custom ML model. Copy the ARN corresponding to your region and specify the same in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the newest boto3\n",
    "get_ipython().run_line_magic('pip', 'install --upgrade boto3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify your valid algorithm ARN\n",
    "# my_algorithm_arn = 'arn:aws:sagemaker:{region}:123456789012:algorithm/{vbffa_algorithm}'\n",
    "my_algorithm_arn = 'arn:aws:sagemaker:{}:{}:algorithm/{}'.format(\n",
    "    'your_region', 'your_aws_account_number', 'your_vbffa_algorithm_label')\n",
    "my_algorithm_arn = 'arn:aws:sagemaker:us-east-1:123456789012:algorithm/vbffa'\n",
    "my_prefix = 'vbffa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Prepare relevant environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python packages\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "# remind\n",
    "print('Wait for Sagemaker values assigned to TWO important variables: my_bucket and my_role.\\n')\n",
    "\n",
    "# sagemaker session\n",
    "my_session = sagemaker.session.Session()\n",
    "\n",
    "# sagemaker attributes\n",
    "my_bucket = my_session.default_bucket()\n",
    "my_role = sagemaker.session.get_execution_role()\n",
    "\n",
    "# review\n",
    "print('my_bucket = {}'.format(my_bucket))\n",
    "print('my_role = {}'.format(my_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this Sagemaker machine learning ('ml') notebook example, following S3 folders are expected to be in place:\n",
    "\n",
    "1. {my_bucket}/{my_prefix}/input/data/train/\n",
    "1. {my_bucket}/{my_prefix}/input/data/inference/\n",
    "1. {my_bucket}/{my_prefix}/model/\n",
    "1. {my_bucket}/{my_prefix}/output/data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 paths\n",
    "my_input_data_train_path = 's3://{}/{}/input/data/train'.format(my_bucket, my_prefix)\n",
    "my_model_path = 's3://{}/{}/model'.format(my_bucket, my_prefix)\n",
    "my_input_data_infer_path = 's3://{}/{}/input/data/inference'.format(my_bucket, my_prefix)\n",
    "my_output_data_infer_path = 's3://{}/{}/output/data/inference'.format(my_bucket, my_prefix)\n",
    "\n",
    "# vbffa Docker container input data channel\n",
    "input_data_train_channel = 'train'\n",
    "input_data_model_channel = 'model'\n",
    "\n",
    "# aws computing instance type: 'ml.m5.xlarge'\n",
    "my_EC2 = 'ml.m5.xlarge'\n",
    "\n",
    "# input CSV data file name\n",
    "my_input_data_file = 'Weekly_VTS_4Yr.csv'\n",
    "\n",
    "# information available model and endpoint\n",
    "my_model_data = str()  # to be assigned / defined\n",
    "my_model_name = str()  # to be assigned / defined \n",
    "my_endpoint_name = 'my-endpoint'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are re-visiting this demo notebook, and your model training job and/or hyperparameter tuning job (to be defined later) were already run at least once successfully, you can avoid running these job(s) again by copying the resulted Sagemaker s3 path of your trained model artifact data file, model.tar.gz, and/or the s3 path of your tuned model artifact data file, model.tar.gz, to the variables, my_trained_model_data and/or my_tuned_model_data, in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model placeholder\n",
    "# my_trained_model_data = str()\n",
    "my_trained_model_data = str()\n",
    "my_trained_model_name = 'my-trained-model'\n",
    "\n",
    "# AVAILABLE trained model\n",
    "# IF your model is trained and not to be trained again, copy-paste or type the s3 path of\n",
    "# your trained model artifact data file as the value of variable my_trained_model_data\n",
    "# my_trained_model_data = '{my_bucket}/{my_prefix}/model/{some_path}/model.tar.gz'\n",
    "my_trained_model_data = ''\n",
    "\n",
    "# review\n",
    "print('Model artifact data file of trained model:')\n",
    "print(my_trained_model_data)\n",
    "print('Name of trained model:')\n",
    "print(my_trained_model_name)\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "# tuned model placeholder\n",
    "# my_tuned_model_data = str()\n",
    "my_tuned_model_data = str()\n",
    "my_tuned_model_name = 'my-tuned-model'\n",
    "\n",
    "# AVAILABLE tuned model\n",
    "# IF your model is tuned and not to be tuned again, copy-paste or type the s3 path of\n",
    "# your tuned model artifact data file as the value of variable my_tuned_model_data\n",
    "# my_tuned_model_data = '{my_bucket}/{my_prefix}/model/{some_path}/model.tar.gz'\n",
    "my_tuned_model_data = ''\n",
    "\n",
    "# review\n",
    "print('\\nModel artifact data file of tuned model:')\n",
    "print(my_tuned_model_data)\n",
    "print('Name of tuned model:')\n",
    "print(my_tuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Dataset format expected by the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VBfFA (variational Bayesian filtering factor analysis) algorithm/model takes, as input data, multiple time-series data contained in a CSV (comma separated value) data table, in the form of a CSV text-string or a CSV text-file.  \n",
    "    \n",
    "Each row of the data table is for values of an individual time-series (TS). Row header is the label or symbol of the time-series.  \n",
    "  \n",
    "Each column is for values of all time-series at a specific moment in time. Column header is the time-index or time-stamp of the moment. The first data column is for the earliest data point and the last column for the most recent data point. The current version of VBfFA requires equally spaced time-stamps.  \n",
    "    \n",
    "Since VBfFA makes factor analysis on multiple time-series, or a time-series of vector, the input data is essentially in the format of \"Row Time-Series of Column Vector\".  \n",
    "    \n",
    "One of the simplest methods to generate such a CSV text-file is to save a Microsoft Excel spreadsheet as (or into) a CSV file.  \n",
    "    \n",
    "You can also find more information about dataset format in **Usage Information** section of [variational Bayesian filtering factor analysis (VBfFA)](https://aws.amazon.com/marketplace/pp/prodview-[xxx999]=beagle&applicationId=AWSMPContessa)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Configure and visualize training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [sample data](https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/input/Weekly_VTS_4Yr.csv) provided with this product/example is six-year weekly (logarithmic) performances of mutual funds traded in the U.S. invested in equities, fixed income, and commodities. Each row is of an individual mutual fund. Each column is of a specific calendar week in history. The last week (the last column) was the week with a time-stamp as \"2021-12-31\". Following simple steps you can upload this sample data to your AWS S3 folder.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Upload datasets to Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the sample dataset from https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/input/Weekly_VTS_4Yr.csv, and then upload the dataset to\n",
    "  \n",
    "1. {my_bucket}/{my_prefix}/input/data/train/ for training\n",
    "1. {my_bucket}/{my_prefix}/input/data/inference/ for inference\n",
    "  \n",
    "following simple steps can be used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Open webpage https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/input/Weekly_VTS_4Yr.csv\n",
    "1. Click [Raw] option located at top right of the data table\n",
    "1. In the Raw data window, right click [Save as]\n",
    "1. Set local file folder and file name in the \"Save As\" window, then click [Save]\n",
    "  \n",
    "1. Open AWS S3 Console\n",
    "1. Go to S3 folder: {my_bucket}/{my_prefix}/input/data/train/\n",
    "1. Upload the saved local data file to your AWS S3 folder\n",
    "1. Go to S3 folder: {my_bucket}/{my_prefix}/input/data/inference/\n",
    "1. Upload the saved local data file to your AWS S3 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Hyperparameters of the VBfFA algorithm/model are for model learning and inference.  \n",
    "   \n",
    "The parameter, len_leaveout_window >= 0, is the length of the LAST data window containing data points of input vector time-series to be left out for model fitting and inference (to be used later for model validation and/or test).  \n",
    "  \n",
    "Here, we set a larger len_leaveout_window (e.g. 145) for initial model training and re-set a smaller len_leaveout_window (e.g. 132) for second training. The TWO separate model trainings illustrates the capability of VBfFA algorithm to update a trained model when new data points become available.  \n",
    "    \n",
    "You can also find more information about dataset format in **Hyperparameters** section of [variational Bayesian filtering factor analysis (VBfFA)](https://aws.amazon.com/marketplace/pp/prodview-[xxx999]=beagle&applicationId=AWSMPContessa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training oriented format of hyperparameters is presented and utilized in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "# all individual elements must be individual strings\n",
    "my_hyperparam = {\n",
    "    'num_factors': '10',\n",
    "    'error_reduct_target': '0.9',\n",
    "    'num_data_points': '52',\n",
    "    'num_va_iteration': '5',\n",
    "    'len_moving_window': '52',\n",
    "    'ts_standardization': 'exp',\n",
    "    # 'len_leaveout_window': '145',\n",
    "    'len_leaveout_window': '132',\n",
    "    'max_len_output_ts': '156',\n",
    "    'score_target_type': 'S',\n",
    "    'max_predict_step': '3',\n",
    "    'weight_dict': \"dict: {}\".format({'*': 1.0}),\n",
    "    'max_num_ts_add_del' : '2'\n",
    "}\n",
    "\n",
    "# metric_list\n",
    "metric_list = [\n",
    "    'avg_fitvar', 'avg_aggvar', 'avg_zscore',\n",
    "    'avg_bias', 'avg_loglik', 'avg_qstat',\n",
    "    'diff_avg_fitvar', 'diff_avg_aggvar', 'diff_avg_zscore',\n",
    "    'diff_avg_bias', 'diff_avg_loglik', 'diff_avg_qstat']\n",
    "\n",
    "# metrics (all individual elements must be individual strings)\n",
    "my_metrics = list()\n",
    "for metric in metric_list:\n",
    "    my_metrics.append(dict({\n",
    "        'Name': '{}'.format(metric),\n",
    "        'Regex': '{}=(.*?);'.format(metric)\n",
    "    }))\n",
    "\n",
    "# review\n",
    "print('Hyperparameters: my_hyperparam =')\n",
    "print(my_hyperparam)\n",
    "\n",
    "# review\n",
    "print('\\nEvaluation metrics: my_metrics =')\n",
    "print(my_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an estimator object for running a training job\n",
    "# Information on sagemaker.algorithm.AlgorithmEstimator():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html\n",
    "my_estimator = sagemaker.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=my_algorithm_arn,\n",
    "    role=my_role,\n",
    "    instance_count=1,\n",
    "    instance_type=my_EC2,\n",
    "    input_mode='File',\n",
    "    output_path=my_model_path,\n",
    "    base_job_name='my-training-job',\n",
    "    sagemaker_session=my_session,\n",
    "    hyperparameters=my_hyperparam,\n",
    "    model_channel_name='model',\n",
    "    metric_definitions=my_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the above my_estimator object, a method my_estimator.fit(inputs=my_training_input) will be called to either train a new model or update a (existing) trained model.  \n",
    "  \n",
    "Both training and updating a model need input vector time-series data. But, updating a trained model needs an additional input: previous trained model artifacts generated by an earlier model training/updating.  \n",
    "  \n",
    "The argument of fit(), my_training_input, defined in the following cell will make input data (located in s3 path my_input_data_train_path) available in channel \"train\", and make previous trained model artifacts (located in s3 path my_model_path) available in channel \"model\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.inputs.TrainingInput():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html\n",
    "my_training_input = dict(\n",
    "{\n",
    "    input_data_train_channel:\n",
    "        sagemaker.inputs.TrainingInput(\n",
    "            # input vector time-series data\n",
    "            s3_data=my_input_data_train_path,\n",
    "            content_type='text/csv',\n",
    "            s3_data_type='S3Prefix',\n",
    "            input_mode='File'),\n",
    "    \n",
    "    input_data_model_channel:\n",
    "        sagemaker.inputs.TrainingInput(\n",
    "            # previous trained model artifacts\n",
    "            s3_data=my_model_path,\n",
    "            content_type='application/gzip',\n",
    "            s3_data_type='S3Prefix',\n",
    "            input_mode='File')\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, set the boolean indicator, run_training_job, to TRUE, in order to\n",
    "1. run VBfFA model training job\n",
    "1. save model artifacts of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_training_job = True | False\n",
    "run_training_job = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When, training a NEW model, we do NOT have or do NOT need previous trained model artifacts and, therefore, in the argument of fit(), the object my_training_input, we will assign \"s3_data=my_input_data_train_path\" to both channels \"train\" and \"model\".  \n",
    "  \n",
    "During waiting time after setting indicator run_training_job above to TRUE and running model training job in the cell below, you can re-set run_training_job indicator back to FALSE in order to avoid accidentally running model training job again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRUE then train the model and save the result\n",
    "if run_training_job and (len(my_trained_model_data) < 0.5):\n",
    "    \n",
    "    # prompt\n",
    "    print()\n",
    "    print(my_estimator.hyperparameters())\n",
    "    print()\n",
    "    print('Setting a larger value to len_leaveout_window for initial model training')\n",
    "    \n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\n",
    "    # sagemaker.estimator.Estimator().set_hyperparameters(**kwargs)\n",
    "    my_estimator.set_hyperparameters(len_leaveout_window=\"145\")\n",
    "    \n",
    "    # prompt\n",
    "    print()\n",
    "    print(my_estimator.hyperparameters())\n",
    "    print()\n",
    "    \n",
    "    # training a NEW model\n",
    "    # NOTE: CHANG s3 path of input_data_model_channel in order\n",
    "    #       to prevent getting any model data from my_model_path\n",
    "    my_training_input = dict(\n",
    "    {\n",
    "        input_data_train_channel:\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                # input vector time-series data\n",
    "                s3_data=my_input_data_train_path,\n",
    "                content_type='text/csv',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File'),\n",
    "        \n",
    "        input_data_model_channel:\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                # NOT previous trained model artifacts\n",
    "                s3_data=my_input_data_train_path,\n",
    "                content_type='application/gzip',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File')\n",
    "    })\n",
    "    \n",
    "    # remind\n",
    "    print('Train the model. Wait for training job completes with information:')\n",
    "    print('Model data of trained model\\n')\n",
    "    \n",
    "    # Information on sagemaker.algorithm.AlgorithmEstimator().fit()\n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html\n",
    "    my_estimator.fit(\n",
    "        inputs=my_training_input,\n",
    "        wait=True,\n",
    "        logs='All')\n",
    "    \n",
    "    # model data information\n",
    "    my_trained_model_data = my_estimator.model_data\n",
    "    \n",
    "    # review\n",
    "    print('\\nModel data of trained model:')\n",
    "    print(my_trained_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information how to visualize metrics during the process, see [Easily monitor and visualize metrics while training models on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/easily-monitor-and-visualize-metrics-while-training-models-on-amazon-sagemaker/).\n",
    "\n",
    "You can also open the training job from [Amazon SageMaker console](https://console.aws.amazon.com/sagemaker/home?#/jobs/) and monitor the metrics/logs in **Monitor** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Update a model with \"trained model retrieval\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variational Bayesian filtering factor analysis (VBfFA) model is continuously fitted or updated by filtering: when a new data point becoming available after the model being fitted, the model is updated based on this new data point and the existing trained model.  \n",
    "   \n",
    "The trained model artifact data file, e.g. {my_bucket}/{my_prefix}/model/{some_path}/model.tar.gz, in this example, generated at the end of the training process contains all model artifacts based on the input vector time-series data points earmarked as available for the model training.  \n",
    "    \n",
    "When new data points of the time-series become available after the training ended, an inference process with the new data and trained model will make an additional model fitting and updating first, and then make inference based on the fully updated model artifacts.  \n",
    "     \n",
    "But the inference process does not generate a new model.tar.gz file to be utilized later again.  \n",
    "  \n",
    "Fortunately, the argument, inputs=my_training_input, of the model training method, sagemaker.algorithm.AlgorithmEstimator().fit(), is able to implement a \"trained model retrieval\" mechanism for updating a trained model: both (a) the newly available data points and (b) the previous trained model artifacts can be fed together into the model updating process.  \n",
    "  \n",
    "The argument of fit(), my_training_input, defined in the following cell will make input data (located in s3 path my_input_data_train_path) available in channed \"train\", and make previous trained model artifacts (located in s3 path my_model_path) available in channel \"model\". The only requirement for a successful trained model retrieval process is to keep only the latest model data file and delete the older one(s).  \n",
    "  \n",
    "This way, an updated trained model data file, model.tar.gz, will be generated containing the newly updated model artifacts.  \n",
    "     \n",
    "In the VBfFA modeling example shown in this notebook, we can delete older model.tart.gz file(s) in the s3 model path, my_model_path = 's3://{}/{}/model'.format(my_bucket, my_prefix), only keep the latest model data file. And then, run the model fitting function, my_estimator.fit(), again.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRUE then train the model and save the result\n",
    "if run_training_job:\n",
    "    \n",
    "    print()\n",
    "    print(my_estimator.hyperparameters())\n",
    "    print()\n",
    "    print('Now, re-setting a smaller value to len_leaveout_window for updating model')\n",
    "    \n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html\n",
    "    # sagemaker.estimator.Estimator().set_hyperparameters(**kwargs)\n",
    "    my_estimator.set_hyperparameters(len_leaveout_window=\"132\")\n",
    "    \n",
    "    # prompt\n",
    "    print()\n",
    "    print(my_estimator.hyperparameters())\n",
    "    print()\n",
    "    \n",
    "    # updating a TRAINED model\n",
    "    # NOTE: KEEP ONLY the last model data in s3 model path of my_model_path\n",
    "    # NOTE: DELETE ALL older model data in s3 model path of my_model_path\n",
    "    my_training_input = dict(\n",
    "    {\n",
    "        input_data_train_channel:\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                # input vector time-series data\n",
    "                s3_data=my_input_data_train_path,\n",
    "                content_type='text/csv',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File'),\n",
    "        \n",
    "        input_data_model_channel:\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                # previous trained model artifacts\n",
    "                s3_data=my_model_path,\n",
    "                content_type='application/gzip',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File')\n",
    "    })\n",
    "    \n",
    "    # remind\n",
    "    print('Train the model. Wait for training job completes with information:')\n",
    "    print('Model data of trained model\\n')\n",
    "    \n",
    "    # Information on sagemaker.algorithm.AlgorithmEstimator().fit()\n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html\n",
    "    my_estimator.fit(\n",
    "        inputs=my_training_input,\n",
    "        wait=True,\n",
    "        logs='All')\n",
    "    \n",
    "    # model data information\n",
    "    my_trained_model_data = my_estimator.model_data\n",
    "    \n",
    "    # review\n",
    "    print('\\nModel data of trained model:')\n",
    "    print(my_trained_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tune your model (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Tuning guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling and/or predicting different sets of multiple time-series require different values of hyperparameters: num_factors, error_reduct_target, num_data_points, num_va_iteration, and len_moving_window. \n",
    "   \n",
    "Therefore, decisions on specific (integer or decimal) values of these hyperparameters need to be made before making meaningful training and inference. There are a variety of commonly practiced methods to estimate the appropriate hyperparameter values. When using AWS Sagemaker, it is natural to use Sagemaker's HyperparameterTuner class to search for appropriate hyperparameter values which result in better forecasts.  \n",
    "   \n",
    "For information about Automatic model tuning, also see [Perform Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "# all individual elements must be individual strings\n",
    "my_hyperparam = {\n",
    "    'num_factors': '10',\n",
    "    'error_reduct_target': '0.9',\n",
    "    'num_data_points': '52',\n",
    "    'num_va_iteration': '5',\n",
    "    'len_moving_window': '52',\n",
    "    'ts_standardization': 'exp',\n",
    "    # 'len_leaveout_window': '145',\n",
    "    'len_leaveout_window': '132',\n",
    "    'max_len_output_ts': '156',\n",
    "    'score_target_type': 'S',\n",
    "    'max_predict_step': '3',\n",
    "    'weight_dict': \"dict: {}\".format({'*': 1.0}),\n",
    "    'max_num_ts_add_del' : '2'\n",
    "}\n",
    "\n",
    "# metric_list\n",
    "metric_list = [\n",
    "    'avg_fitvar', 'avg_aggvar', 'avg_zscore',\n",
    "    'avg_bias', 'avg_loglik', 'avg_qstat',\n",
    "    'diff_avg_fitvar', 'diff_avg_aggvar', 'diff_avg_zscore',\n",
    "    'diff_avg_bias', 'diff_avg_loglik', 'diff_avg_qstat']\n",
    "\n",
    "# metrics (all individual elements must be individual strings)\n",
    "my_metrics = list()\n",
    "for metric in metric_list:\n",
    "    my_metrics.append(dict({\n",
    "        'Name': '{}'.format(metric),\n",
    "        'Regex': '{}=(.*?);'.format(metric)\n",
    "    }))\n",
    "\n",
    "# create an estimator object for running a training job\n",
    "# Information on sagemaker.algorithm.AlgorithmEstimator():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/training/algorithm.html\n",
    "my_estimator = sagemaker.algorithm.AlgorithmEstimator(\n",
    "    algorithm_arn=my_algorithm_arn,\n",
    "    role=my_role,\n",
    "    instance_count=1,\n",
    "    instance_type=my_EC2,\n",
    "    input_mode='File',\n",
    "    output_path=my_model_path,\n",
    "    base_job_name='my-training-job',\n",
    "    sagemaker_session=my_session,\n",
    "    hyperparameters=my_hyperparam,\n",
    "    model_channel_name='model',\n",
    "    metric_definitions=my_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Define tuning configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible ranges of appropriate hyperparameter values depend on specific dataset at hand. For the sample dataset used in this example, a set of reasonable ranges of hyperparameter values are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.parameter.IntegerParameter():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html\n",
    "tuning_hyperpar_range_example = dict({\n",
    "    'num_factors':\n",
    "        sagemaker.parameter.IntegerParameter(\n",
    "        min_value=5, max_value=50, scaling_type='Auto'),\n",
    "    'error_reduct_target':\n",
    "        sagemaker.parameter.ContinuousParameter(\n",
    "        min_value=0.75, max_value=0.99, scaling_type='Auto'),\n",
    "    'num_data_points':\n",
    "        sagemaker.parameter.IntegerParameter(\n",
    "        min_value=10, max_value=100, scaling_type='Auto'),\n",
    "    'num_va_iteration':\n",
    "        sagemaker.parameter.IntegerParameter(\n",
    "        min_value=2, max_value=10, scaling_type='Auto'),\n",
    "    'len_moving_window':\n",
    "        sagemaker.parameter.IntegerParameter(\n",
    "        min_value=52, max_value=157, scaling_type='Auto')\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural seasonality of time-series and some \"rule of thumb for choices\" may be utilized to focus on a few reasonable values within reasonable ranges. Following example can be used for a simpler model tuning.\n",
    "\n",
    "For general information about AWS SageMaker Hyperparameter Tuning, referred to [How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html) and [Define Hyperparameter Ranges](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.parameter.CategoricalParameter():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/training/parameter.html\n",
    "my_hyperparam_range = dict({\n",
    "    'num_factors':\n",
    "        sagemaker.parameter.CategoricalParameter(['5', '10', '20', '30']),\n",
    "    'error_reduct_target':\n",
    "        sagemaker.parameter.CategoricalParameter(['0.85', '0.9', '0.95']),\n",
    "    'num_data_points':\n",
    "        sagemaker.parameter.CategoricalParameter(['26', '52', '78']),\n",
    "    'num_va_iteration':\n",
    "        sagemaker.parameter.CategoricalParameter(['5', '10']),\n",
    "    'len_moving_window':\n",
    "        sagemaker.parameter.CategoricalParameter(['52', '65'])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different modeling and inference applications need to use different metrics to measure relevant goodness of fit.  \n",
    "  \n",
    "With the VBfFA (variational Bayesian filtering factor analysis) model, we try to predict a number of unobserved common factors underlying a set of large number of related time-series.  \n",
    "  \n",
    "covariance between time-series can be estimated by common factors, while variance of time-series can be estimated by both common factors and residual variance. therefore, evaluation scores of time-varying or time-deplendent variance-covariance matrix of the time-series can serve as measures of goodness of fit of a factor model, such as our VBfFA model.  \n",
    "  \n",
    "In this example, we will use log-likelihood of variance of aggregated time-series as the evaluation score.\n",
    "detailes are discussed in the publication on the VBfFA model, https://doi.org/10.1080/14697688.2016.1268708, or, https://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1268708, or,         https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Publication.pdf,         or https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Manuscript.pdf.\n",
    "  \n",
    "For general information about AWS SageMaker Metrics, referred to [Define Metrics](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available choices for objective tuning metric\n",
    "print('Available model evaulation metrics:')\n",
    "print(my_metrics)\n",
    "\n",
    "# name of objective tuning metric\n",
    "my_objective_metric = my_metrics[4]['Name']\n",
    "\n",
    "# review\n",
    "print('\\nObjective tuning metric')\n",
    "print(my_objective_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, minimizing error and/or maximizing similarity are desirable tuning directions. Therefore, we will maximize our objective metric, projection coefficient, in this hyperparameter tuning example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direction of hyperparameter optimization\n",
    "my_objective_type = 'Maximize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Run a model tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up hyperparameter tuning job\n",
    "# Information on sagemaker.tuner.HyperparameterTuner():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html\n",
    "my_tuner = sagemaker.tuner.HyperparameterTuner(\n",
    "    estimator=my_estimator,\n",
    "    objective_metric_name=my_objective_metric,\n",
    "    hyperparameter_ranges=my_hyperparam_range,\n",
    "    objective_type=my_objective_type,\n",
    "    max_jobs=1,\n",
    "    max_parallel_jobs=1,\n",
    "    base_tuning_job_name='my-tuning-job',\n",
    "    early_stopping_type='Auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, set the boolean indicator, run_tuning_job, to TRUE, in order to\n",
    "1. run hyperparameter optimization job\n",
    "1. save optimal model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_tuning_job = True | False\n",
    "run_tuning_job = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During waiting time after setting indicator run_tuning_job above to TRUE and running hyperparameter tuning job in the cell below, you can re-set run_tuning_job indicator back to FALSE in order to avoid accidentally running hyperparameter tuning job again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRUE then optimize model and save the result\n",
    "if run_tuning_job and (len(my_tuned_model_data) < 0.5):\n",
    "    \n",
    "    # Tuning Job\n",
    "    # NOTE: CHANG s3 path of input_data_model_channel inorder\n",
    "    #       to prevent getting any model data from my_model_path\n",
    "    my_training_input = dict(\n",
    "    {\n",
    "        input_data_train_channel:\n",
    "            # input vector time-series data\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                s3_data=my_input_data_train_path,\n",
    "                content_type='text/csv',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File'),\n",
    "        \n",
    "        input_data_model_channel:\n",
    "            # NOT previous trained model artifacts\n",
    "            sagemaker.inputs.TrainingInput(\n",
    "                s3_data=my_input_data_train_path,\n",
    "                content_type='application/gzip',\n",
    "                s3_data_type='S3Prefix',\n",
    "                input_mode='File')\n",
    "    })\n",
    "    \n",
    "    # remind\n",
    "    print('Tune the model. Wait for tuning job completes with information:')\n",
    "    print('Model data of tuned model\\n')\n",
    "    \n",
    "    # tuning and waiting\n",
    "    # Information on sagemaker.tuner.HyperparameterTuner().fit():\n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html\n",
    "    my_tuner.fit(\n",
    "        inputs=my_training_input)\n",
    "    my_tuner.wait()\n",
    "    \n",
    "    # get tuned model and artfacts of the tuned model\n",
    "    # Information on sagemaker.tuner.HyperparameterTuner().best_estimator():\n",
    "    # https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html\n",
    "    my_tuned_estimator = my_tuner.best_estimator()\n",
    "    \n",
    "    # optimized hyperparameters\n",
    "    my_tuned_hyperparam = my_tuned_estimator.hyperparameters()\n",
    "    \n",
    "    # optimal model artfacts\n",
    "    my_tuned_model_data = my_tuned_estimator.model_data\n",
    "    \n",
    "    # review\n",
    "    print('\\nTuned hyperparameters:')\n",
    "    print(my_tuned_hyperparam)\n",
    "    \n",
    "    # review\n",
    "    print('\\nModel data of tuned model:')\n",
    "    print(my_tuned_model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As recommended by AWS Sagemaker Team, once you have completed a tuning job, (or even while the job is still running) you can [clone and use this notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) to analyze the results to understand how each hyperparameter effects the quality of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deploy model and verify results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Trained or tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available trained model\n",
    "if len(my_trained_model_data) > len('s3://.tar.gz'):\n",
    "    my_model_data = my_trained_model_data\n",
    "    my_model_name = my_trained_model_name\n",
    "\n",
    "# available tuned model\n",
    "if len(my_tuned_model_data) > len('s3://.tar.gz'):\n",
    "    my_model_data = my_tuned_model_data\n",
    "    my_model_name = my_tuned_model_name\n",
    "\n",
    "# Information on sagemaker.model.ModelPackage():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html\n",
    "my_model = sagemaker.model.ModelPackage(\n",
    "    role=my_role,\n",
    "    model_data=my_model_data,\n",
    "    algorithm_arn=my_algorithm_arn,\n",
    "    name=my_model_name\n",
    ")\n",
    "\n",
    "# review\n",
    "print('Name of model:')\n",
    "print(my_model_name)\n",
    "\n",
    "# review\n",
    "print('\\nArtifacts of model:')\n",
    "print(my_model_data)\n",
    "\n",
    "# review\n",
    "print('\\nModel pacakge')\n",
    "print(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Deploy trained or tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind\n",
    "print('Start endpoint for inference. Wait for endpoint becomes ready')\n",
    "\n",
    "# Information on sagemaker.model.Model().deploy():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html\n",
    "my_endpoint = my_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=my_EC2,\n",
    "    endpoint_name=my_endpoint_name\n",
    ")\n",
    "\n",
    "# review\n",
    "print('\\nSagemaker endpoint, ' + my_endpoint_name + ', is ready NOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.serializers.IdentitySerializer():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/serializers.html\n",
    "my_serializer = sagemaker.serializers.IdentitySerializer()\n",
    "\n",
    "# Information on sagemaker.deserializers.StreamDeserializer():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/deserializers.html\n",
    "my_deserializer = sagemaker.deserializers.StreamDeserializer()\n",
    "\n",
    "# Predictor\n",
    "# Information on sagemaker.predictor.Predictor():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html\n",
    "my_predictor = sagemaker.predictor.Predictor(\n",
    "    endpoint_name=my_endpoint_name,\n",
    "    sagemaker_session=my_session,\n",
    "    serializer=my_serializer,\n",
    "    deserializer=my_deserializer\n",
    ")\n",
    "\n",
    "# review\n",
    "print(my_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Create input payload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input payload can be created by following functions of the class [S3 Utilities](https://sagemaker.readthedocs.io/en/stable/api/utility/s3.html)\n",
    "\n",
    "1. **sagemaker.s3.s3_path_join(*args)**: similarly to os.path.join()\n",
    "1. **sagemaker.s3.S3Downloader.read_file(s3_uri, sagemaker_session=None)**: returns the contents of an s3 uri file body as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data file for inference\n",
    "my_infer_input_file = sagemaker.s3.s3_path_join(\n",
    "    my_input_data_infer_path,\n",
    "    my_input_data_file)\n",
    "\n",
    "# CSV data: string\n",
    "my_infer_input_str = sagemaker.s3.S3Downloader.read_file(\n",
    "    my_infer_input_file, \n",
    "    sagemaker_session=my_session)\n",
    "\n",
    "# CSV data: byte stream object\n",
    "my_inference_input_obj = my_infer_input_str.encode()\n",
    "\n",
    "# review\n",
    "print('my_infer_input_file:')\n",
    "print(my_infer_input_file + '\\n')\n",
    "\n",
    "# review\n",
    "print('my_infer_input_str: ' + str(type(my_infer_input_str)))\n",
    "print('my_inference_input_obj: ' + str(type(my_inference_input_obj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.predictor.Predictor().predict():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html\n",
    "my_predict = my_predictor.predict(\n",
    "    data=my_inference_input_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review\n",
    "print('Output of real-time inference:')\n",
    "print(my_predict)\n",
    "\n",
    "# review\n",
    "# Information on botocore.response.StreamingBody()\n",
    "# https://botocore.amazonaws.com/v1/documentation/api/latest/reference/response.html\n",
    "print('\\nReal-time prediction')\n",
    "print(my_predict[0].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate it to avoid being charged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.predictor.Predictor().delete_endpoint():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html\n",
    "my_predictor.delete_endpoint(\n",
    "    delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Perform batch inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. Batch transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default inference ENV variables\n",
    "my_ENV = dict({\n",
    "    'MODELOUTPUT': 'fit_output'\n",
    "})\n",
    "\n",
    "# available output type\n",
    "output_type_choice = dict({\n",
    "    1: 'text/csv',\n",
    "    2: 'application/json'\n",
    "})\n",
    "\n",
    "# output type\n",
    "output_type = output_type_choice[\n",
    "    2\n",
    "]\n",
    "\n",
    "# Information sagemaker.transformer.Transformer():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html\n",
    "my_transformer = sagemaker.transformer.Transformer(\n",
    "    model_name=my_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=my_EC2,\n",
    "    output_path=my_output_data_infer_path,\n",
    "    accept=output_type,\n",
    "    env=my_ENV,\n",
    "    sagemaker_session=my_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Batch-transform job input file is located in the S3 folder: {my_bucket}/{my_prefix}/input/data/inference/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.inputs.TransformInput():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html\n",
    "my_transform_data_path = my_input_data_infer_path\n",
    "my_transform_data_type = 'S3Prefix'\n",
    "my_transform_content_type = 'text/csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remind\n",
    "print('Run batch transform. Wait for transform job completes with information:')\n",
    "print('Batch transform output path')\n",
    "\n",
    "# Information on sagemaker.transformer.Transformer().transform():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/transformer.html\n",
    "my_transformer.transform(\n",
    "    data=my_transform_data_path,\n",
    "    data_type=my_transform_data_type,\n",
    "    content_type=my_transform_content_type,\n",
    "    # wait=False,\n",
    "    logs=True\n",
    ")\n",
    "\n",
    "# wait\n",
    "my_transformer.wait()\n",
    "\n",
    "# output is available on following path\n",
    "my_transform_output_path = my_transformer.output_path\n",
    "print('Batch transform output path:')\n",
    "print(my_transform_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can display and review output generated by the batch transform job available in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform output file name = {input_data_file}.csv.out\n",
    "my_transform_output_file = my_input_data_file + '.out'\n",
    "\n",
    "# data file for inference\n",
    "my_inference_file = sagemaker.s3.s3_path_join(\n",
    "    my_transform_output_path,\n",
    "    my_transform_output_file)\n",
    "\n",
    "# CSV data string\n",
    "my_inference = sagemaker.s3.S3Downloader.read_file(\n",
    "    my_inference_file, \n",
    "    sagemaker_session=my_session)\n",
    "\n",
    "# review\n",
    "print('Output of batch transform job:\\n')\n",
    "print(my_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may change the transform output file name to keep the file from being overwritten.\n",
    "\n",
    "Open AWS S3 Console, go to the batch transform output path shown above, re-name the file \"{inference_input_data_file_name}.csv.out\" to\n",
    "1. \"fit_output.csv\", if accept = output_type = 'text/csv', or\n",
    "1. \"fit_output.json\", if accept = output_type = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Delete the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a batch inference. IF you plan to review the trained or tuned model structure by using Transformer as demonstrated later, do NOT run the cell below. Otherwise, you can delete the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need more batch transform?\n",
    "more_batch_transform = True\n",
    "\n",
    "# Information on sagemaker.session.Session().delete_model():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "if not more_batch_transform:\n",
    "    my_session.delete_model(my_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model review by using Transformer (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. VBfFA predictions and goodness scores of the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels**  \n",
    "  \n",
    "1. 'fit_output': Output time-series from fitting variational Bayesian filtering factor analysis (VBfFA) model\n",
    "1. 'posterior': Posterior of variational Bayesian filtering as of the last time stamp of sample data vector time-series\n",
    "1. 'pred_output': Output time-series from prediction by fitted variational Bayesian filtering factor analysis (VBffA) model\n",
    "1. 'last_pred': Predictions as of the last output time-stamp by fitted variational Bayesian filtering factor analysis (VBffA) model\n",
    "1. 'eval_score': Variational Bayesian filtering factor analysis (VBfFA) model evaluation score time-series\n",
    "1. 'eval_stats': Variational Bayesian filtering factor analysis (VBfFA) model evaluation statistics, by averages over time of VBffA model evaluation score time-series\n",
    "1. 'diff_score': Arithmetic or logarithmic difference of VBfFA model evaluation score time-series minus OBS model evaluation score time-series\n",
    "1. 'diff_stats': VBfFA vs. OBS model evaluation statistics, by averages over time of arithmetic or logarithmic difference of VBfFA model evaluation score time-series minus OBS model evaluation score time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Select prediction or score for review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained or tuned VBfFA predictions or scores can be reviewed using Transformer with specific values of environment variable,\n",
    "MODELOUTPUT, and choice of output type.  \n",
    "   \n",
    "Choices of values of model output and output type are:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available choices for MODELOUTPUT\n",
    "model_output_choice = dict({\n",
    "    1: 'fit_output',\n",
    "    2: 'posterior',\n",
    "    3: 'pred_output',\n",
    "    4: 'last_pred',\n",
    "    5: 'eval_score',\n",
    "    6: 'eval_stats',\n",
    "    7: 'diff_score',\n",
    "    8: 'diff_stats'\n",
    "})\n",
    "\n",
    "# available choices for output type\n",
    "output_type_choice = dict({\n",
    "    1: 'text/csv',\n",
    "    2: 'application/json'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can make any valid pair of choices as exemplified as in following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice for MODELOUTPUT (an integer between 1 and 8)\n",
    "model_output = model_output_choice[\n",
    "    3\n",
    "]\n",
    "\n",
    "# output type\n",
    "output_type = output_type_choice[\n",
    "    2\n",
    "]\n",
    "\n",
    "# review\n",
    "print('model_output = ' + model_output)\n",
    "print('output_type = ' + output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Model output review with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV variables\n",
    "my_ENV = dict({\n",
    "    'MODELOUTPUT': model_output})\n",
    "\n",
    "# sagemaker.transformer.Transformer()\n",
    "my_transformer = sagemaker.transformer.Transformer(\n",
    "    model_name=my_model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=my_EC2,\n",
    "    output_path=my_output_data_infer_path,\n",
    "    accept=output_type,\n",
    "    env=my_ENV,\n",
    "    sagemaker_session=my_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sagemaker.inputs.TransformInput()\n",
    "my_transform_data_path = my_input_data_infer_path\n",
    "my_transform_data_type = 'S3Prefix'\n",
    "my_transform_content_type = 'text/csv'\n",
    "\n",
    "# remind\n",
    "print('Run batch transform. Wait for transform job completes with information:')\n",
    "print('Batch transform output path')\n",
    "\n",
    "# sagemaker.transformer.Transformer()\n",
    "my_transformer.transform(\n",
    "    data=my_transform_data_path,\n",
    "    data_type=my_transform_data_type,\n",
    "    content_type=my_transform_content_type,\n",
    "    # wait=False,\n",
    "    logs=True)\n",
    "\n",
    "# wait\n",
    "my_transformer.wait()\n",
    "\n",
    "# output is available on following path\n",
    "my_transform_output_path = my_transformer.output_path\n",
    "print('Batch transform output path:')\n",
    "print(my_transform_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can display and review output generated by the batch transform job available in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform output file name = {input_data_file}.csv.out\n",
    "my_transform_output_file = my_input_data_file + '.out'\n",
    "\n",
    "# data file for inference\n",
    "my_inference_file = sagemaker.s3.s3_path_join(\n",
    "    my_transform_output_path,\n",
    "    my_transform_output_file)\n",
    "\n",
    "# CSV data string\n",
    "my_inference = sagemaker.s3.S3Downloader.read_file(\n",
    "    my_inference_file, \n",
    "    sagemaker_session=my_session)\n",
    "\n",
    "# display\n",
    "print('Selected output:\\n')\n",
    "print(my_inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may change the selected output file name to keep the file from being overwritten.\n",
    "\n",
    "Open AWS S3 Console, go to the batch transform output path shown above, re-name the file \"{inference_input_data_file_name}.csv.out\" to\n",
    "1. \"{model_output}.csv\", if accept = output_type = 'text/csv', or\n",
    "1. \"{model_output}.json\", if accept = output_type = 'application/json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Delete endpoint and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.predictor.Predictor().delete_endpoint():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/predictors.html\n",
    "my_predictor.delete_endpoint(\n",
    "    delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information on sagemaker.session.Session().delete_model():\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/utility/session.html\n",
    "my_session.delete_model(my_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2. Unsubscribe to the listing (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to unsubscribe to the algorithm, follow these steps. Before you cancel the subscription, ensure that you do not have any [deployable model](https://console.aws.amazon.com/sagemaker/home#/models) created from the model package or using the algorithm. Note - You can find this information by looking at the container name associated with the model. \n",
    "\n",
    "**Steps to unsubscribe to product from AWS Marketplace**:  \n",
    "\n",
    "1. Navigate to __Machine Learning__ tab on [__Your Software subscriptions page__](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust)\n",
    "2. Locate the listing that you want to cancel the subscription for, and then choose __Cancel Subscription__  to cancel the subscription.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
