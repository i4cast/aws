{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, tune, deploy and review ML algorithm/model VBfFA (Variational Bayesian filtering Factor Analysis) from AWS Marketplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the algorithm  \n",
    "  \n",
    "The variational Bayesian filtering factor analysis (VBfFA) algorithm/model is a filter (of dimension-reduction, or rank-reduction) to extract a number of ever-evolving unobserved common factors, or signals from common sources, underlying and influencing a large number of related time-series data.\n",
    "  \n",
    "Relevant examples of time-series data include: economic indicators in a nation, a region, or an international economic sector; prices of assets in a national, regional or global asset class(es), or marketplace; performance measurement time-series with various lags related to a business marketing campaign; and time-series signals from an array of radar or sonar sensors tracking a number of moving targets; etc.\n",
    "  \n",
    "By applying (variational) Bayesian filtering (instead of traditional moving/rolling data windows for frequentist time-dependent analysis), the VBfFA algorithm is able to update predictions with only the newly arrived time-series data point (instead of all data points in the data window); to speed up, as a result, real-time prediction process; to predict underlying changes in time-series early; and to avoid over- or under-fitting by setting a reasonable “estimation error reduction target”.\n",
    "  \n",
    "In addition to serving as a stand-alone filtering package for time-varying factor analysis on multiple time-series data, the VBfFA algorithm will be employed as the underlying factor analysis engine of other machine learning packages here introduced earlier by i4cast LLC: LMDFM (long memory dynamic factor model); YWpcAR (Yule-Walker-PCA autoregressive model); LMVAR (long memory vector autoregressive model); and CTVARF (continuously trained vector autoregressive forecast model).\n",
    "  \n",
    "Current version of the VBfFA algorithm estimates: time-series of posterior from the (variational) Bayesian filtering; predicted values and time-dependent variances of common factors (or common signals) and time-dependent factor loadings; predicted time-dependent (or time-varying) variance-covariance matrix of multiple time-series; and evaluation scores of the predicted time-series of variance-covariance matrix.\n",
    "  \n",
    "A notable application of the VBfFA estimates, detecting timely changes in time-dependent variance-covariance matrix of financial instruments, presented in a particular form, can serve as an early warning system indicating potential troubles in the financial market.\n",
    "  \n",
    "This VBfFA module implements VBfFA formulation published by Figure 1 through Figure 4 in a paper in journal of Quantitative Finance, https://doi.org/10.1080/14697688.2016.1268708, or, https://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1268708, or, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Publication.pdf, or manuscript of the publication, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Manuscript.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publications on variational Bayesian filtering factor analysis (VBfFA) modeling  \n",
    "\n",
    "HFL & CF. (2017) \"Online learning of time-varying stochastic factor structure by variational sequential Bayesian factor analysis\", Quantitative Finance, Vol. 17 (8), pp. 1277-1304. Publication: https://doi.org/10.1080/14697688.2016.1268708, or, https://www.tandfonline.com/doi/abs/10.1080/14697688.2016.1268708, or, https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Publication.pdf. Manuscript: https://github.com/i4cast/aws/blob/main/variational_Bayesian_filtering_factor_analysis/publication/VBfFA_Manuscript.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook\n",
    "   \n",
    "This sample notebook shows you how to train, tune, deploy and understand a custom ML algorithm/model:\n",
    "[variational Bayesian filtering factor analysis (VBfFA)]\n",
    "(https://aws.amazon.com/marketplace/pp/prodview-[xxx999]=beagle&applicationId=AWSMPContessa)\n",
    ", guided by common practices to [Use Algorithm and Model Package Resources]\n",
    "(https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-mkt-buy.html).\n",
    "   \n",
    "> **Note**: This is a reference notebook and it cannot run unless you make changes suggested in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites\n",
    "\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**\n",
    "1. Some hands-on experience using [Amazon SageMaker](https://aws.amazon.com/sagemaker/).\n",
    "1. To use this algorithm successfully, ensure that:\n",
    "    1. Either your IAM role has these three permissions and you have authority to make AWS Marketplace subscriptions in the AWS account used: \n",
    "        1. **aws-marketplace:ViewSubscriptions**\n",
    "        1. **aws-marketplace:Unsubscribe**\n",
    "        1. **aws-marketplace:Subscribe**  \n",
    "    1. or your AWS account has a subscription to \n",
    "[variational Bayesian filtering factor analysis (VBfFA)]\n",
    "(https://aws.amazon.com/marketplace/pp/prodview-[xxx999]=beagle&applicationId=AWSMPContessa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contents\n",
    "\n",
    "1. [Subscribe to the algorithm](#1.-Subscribe-to-the-algorithm)\n",
    "    1. [Subscription](#1.1.-Subscription)\n",
    "    1. [Prepare relevant environment](#1.2.-Prepare-relevant-environment)\n",
    "1. [Prepare dataset](#2.-Prepare-dataset)\n",
    "    1. [Dataset format expected by the algorithm](#2.1.-Dataset-format-expected-by-the-algorithm)\n",
    "    1. [Configure and visualize training dataset](#2.2.-Configure-and-visualize-training-dataset)\n",
    "    1. [Upload datasets to Amazon S3](#2.3.-Upload-datasets-to-Amazon-S3)\n",
    "1. [Train a machine learning model](#3.-Train-a-machine-learning-model)\n",
    "    1. [Set hyperparameters](#3.1.-Set-hyperparameters)\n",
    "    1. [Train a model](#3.2.-Train-a-model)\n",
    "    1. [Update a model](#3.3.-Update-a-model-with-\"trained-model-retrieval\")\n",
    "1. [Tune your model (optional)](#4.-Tune-your-model-(optional))\n",
    "    1. [Tuning Guidelines](#4.1.-Tuning-guidelines)\n",
    "    1. [Define Tuning configuration](#4.2.-Define-tuning-configuration)\n",
    "    1. [Run a model tuning job](#4.3.-Run-a-model-tuning-job)\n",
    "1. [Deploy model and verify results](#5.-Deploy-model-and-verify-results)\n",
    "    1. [Trained or tuned model](#5.1.-Trained-or-tuned-model)\n",
    "    1. [Deploy trained or tuned model](#5.2.-Deploy-trained-or-tuned-model)\n",
    "    1. [Create input payload](#5.3.-Create-input-payload)\n",
    "    1. [Perform real-time inference](#5.4.-Perform-real-time-inference)\n",
    "1. [Perform Batch inference](#6.-Perform-batch-inference)\n",
    "    1. [Batch transform](#6.1.-Batch-transform)\n",
    "    1. [Delete the model](#6.2.-Delete-the-model)\n",
    "1. [Model review by using Transformer (optional)](#7.-Model-review-by-using-Transformer-(optional))\n",
    "    1. [VBfFA predictions and goodness scores](#7.1.-VBfFA-predictions-and-goodness-scores-of-the-predictions)\n",
    "    1. [Select prediction or score for review](#7.2.-Select-prediction-or-score-for-review)\n",
    "    1. [Model output review with Transformer](#7.3.-Model-output-review-with-Transformer)\n",
    "1. [Clean-up](#8.-Clean-up)\n",
    "    1. [Delete endpoint and model](#8.1.-Delete-endpoint-and-model)\n",
    "    1. [Unsubscribe to the listing (optional)](#8.2.-Unsubscribe-to-the-listing-(optional))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage instructions\n",
    "\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sagemaker Notebook\n",
    "\n",
    "For readers who like to review how to use Sagemaker Notebook in general, following Sagemaker documentation pages are best resources.  \n",
    "    [Get Started with Amazon SageMaker Notebook Instances](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-console.html)  \n",
    "    [Step 1: Create an Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-setup-working-env.html)  \n",
    "    [Step 2: Create a Jupyter Notebook](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-prepare.html)  \n",
    "    [Step 3: Download, Explore, and Transform a Dataset](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-preprocess-data.html)  \n",
    "    [Step 4: Train a Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-train-model.html)  \n",
    "    [Step 5: Deploy the Model to Amazon EC2](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-model-deployment.html)  \n",
    "    [Step 6: Evaluate the Model](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html)  \n",
    "    [Step 7: Clean Up](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
